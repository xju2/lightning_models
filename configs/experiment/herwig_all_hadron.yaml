# @package _global_

# to execute this experiment run:
# python src/train.py experiment=herwig_all_hadron
## to add a logger
# python src/train.py experiment=herwig_all_hadron logger=wandb

## with training techniques
# python src/train.py experiment=herwig_all_hadron logger=wandb +trainer.gradient_clip_val=0.5

defaults:
  - override /datamodule: herwig.yaml
  - override /model: cgan.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["herwig", "allhadrons"]

seed: 12345

trainer:
  max_epochs: 100
  val_check_interval: 20
  
## override /datamodule:
datamodule:
  batch_size: 10_000
  core_dataset:
    train_val_test_split: [5_000_000, 20_000, 20_000]


## override /model:
model:
  noise_dim: 64
  generator:
    input_dim: 72
    hidden_dims: [256, 256, 256, 256, 256, 256, 256, 256, 256, 256]

  discriminator:
    encoder_dims: [128, 128, 128, 128, 128, 128, 128, 128]
    decoder_dims: [128, 128, 128, 128, 128, 128, 128, 128]
    word_embedding_dim: 8

  optimizer_generator:
    lr: 0.000005

  optimizer_discriminator:
    lr: 0.001